{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install lxml\n",
    "%pip install nltk beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for web scraping, text processing, and summarization\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update NLTK data path to avoid issues with missing resources\n",
    "nltk.data.path.append(r\"C:/Users/MSI-NB/AppData/Roaming/nltk_data\")\n",
    "\n",
    "# Download necessary NLTK resources (tokenizers and stopwords)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NLTK data path again after downloading\n",
    "nltk.data.path.append(r\"C:/Users/MSI-NB/AppData/Roaming/nltk_data\")\n",
    "\n",
    "# Check the NLTK data paths to confirm correct setup\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the article from Wikipedia using the URL\n",
    "url = 'https://en.wikipedia.org/wiki/Turkey'\n",
    "scraped_data = urllib.request.urlopen(url)\n",
    "\n",
    "# Read the HTML content and decode it as UTF-8\n",
    "article = scraped_data.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using BeautifulSoup with 'lxml' parser\n",
    "parsed_article = bs.BeautifulSoup(article, 'lxml', from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the paragraphs in the article\n",
    "paragraphs = parsed_article.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty string to store article text\n",
    "article_text = \"\"\n",
    "for p in paragraphs:\n",
    "    article_text += p.text  # Extract text from each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text by removing references (e.g., [1], [2], etc.)\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  # Remove reference numbers\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)  # Remove extra spaces\n",
    "\n",
    "# Keep only alphabetic characters (ignoring digits and special symbols)\n",
    "formatted_article_text = re.sub('[^a-zA-ZçğıöşüÇĞİÖŞÜ]', ' ', article_text)\n",
    "\n",
    "# Normalize spaces to avoid multiple consecutive spaces\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Text To Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Text to Sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Manually add NLTK data path for tokenizers\n",
    "nltk.data.path.append('C:/Users/MSI-NB/AppData/Roaming/nltk_data/tokenizers')\n",
    "\n",
    "# Split the cleaned article text into sentences using a regular expression\n",
    "sentence_list = re.split(r'(?<=[.!?]) +', formatted_article_text)\n",
    "\n",
    "# Display the first 5 sentences to check the split\n",
    "print(sentence_list[:5])  # Print the first 5 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Weighted Frequency of Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store word frequencies\n",
    "word_frequencies = {}\n",
    "\n",
    "# Calculate word frequencies, excluding stopwords and non-alphabetic characters\n",
    "for sentence in sentence_list:\n",
    "    for word in sentence.split():\n",
    "        word = word.lower()  # Convert to lowercase for consistency\n",
    "        if word not in stop_words and word.isalpha():  # Exclude stopwords and non-alphabetic words\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest frequency word in the dictionary\n",
    "max_frequency = max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the word frequencies to adjust them relative to the highest frequency\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] = (word_frequencies[word] / max_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Sentence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store sentence scores\n",
    "sentence_scores = {}\n",
    "\n",
    "# Calculate sentence scores based on word frequencies\n",
    "for sentence in sentence_list:\n",
    "    sentence_score = 0\n",
    "    for word in sentence.split():\n",
    "        word = word.lower()\n",
    "        if word in word_frequencies:\n",
    "            sentence_score += word_frequencies[word]  # Add word frequency to the sentence score\n",
    "    sentence_scores[sentence] = sentence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 7 sentences with the highest scores to form the summary\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the summary by joining the top sentences\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Statistics: Calculate the number of sentences and characters before and after summarization\n",
    "original_sentence_count = len(sentence_list)\n",
    "original_character_count = len(article_text)\n",
    "summary_sentence_count = len(summary_sentences)\n",
    "summary_character_count = len(summary)\n",
    "\n",
    "# Print the summary and statistics\n",
    "print(\"Summary:\\n\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nText Statistics:\")\n",
    "print(f\"Number of sentences before summarization: {original_sentence_count}\")\n",
    "print(f\"Number of characters before summarization: {original_character_count}\")\n",
    "print(f\"Number of sentences in the summary: {summary_sentence_count}\")\n",
    "print(f\"Number of characters in the summary: {summary_character_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
